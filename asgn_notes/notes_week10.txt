3/7/16:
	Issues with free space management
		OS must protect data structures used for free space management
		OS must keep in-memory and on-disk structures consistent
			update free list when block removed - change pointer in previous block
			update bit map when block allocated
				set bit[j] in free map to 1 on disk before using block[j] in
				file and setting bit[j] to 1 in memory
			OS crash may leave bit[j] == 1, when block not in use
			OS checks the file system when it boots
		Managing free space is a slowdown in file systems
	Big or small file blocks?
		larger blocks are
			faster - transfer more data per seek
			less efficient - waste space
	What's in a directory?
		Two types of information
			file names
			file metadata (size, timestamps, etc.)
		basic choices for directory information
			store all information in directory
				fixed size entries
				disk addresses and attributes in directory entry
		store names & pointers to index nodes (i-nodes)
	Directory structures
		structure
			linear list of files (often itself stored in a file)
			simple to program
			slow to run
			increase speed by keeping it sorted (insertions are slower)
		hash table: name hashed and looked up in file
			decreases search time: no linear searches
			may be difficult to expand
			can result in collisions (two files hash to same location)
		tree
			fast for searching
			easy to expand
			difficult to do in on-disk directory
		name length
			fixed: easy to program
			variable: more flexible, better for users
	Using links
		A creates a file, and inserts into her directory
		B shares the file by creating a link to it
		A unlinks the file
			B still links to the file
			owner is still A (unless B explicitly changes it)
	Log-structured file systems
		trends in disk & memory
			faster CPUs
			larger memories
		result
			more memory => disk caches can also be larger
			increasing number of read requests can come from cache
			thus, most disk accesses will be writes
		LFS structures entire disk as a log
			all writes initially buffered in memory
			periodically write these to the end of the disk log
			when file opened, locate i-node, then find blocks
	LFS
		divide disk into segments
			write data sequentially to segments
			read data from wherever it’s stored
		periodically, “clean” segments
			go through a segment and copy live data to a new location
			mark the segment as free
	Disk quotas
		quotas limit users' allocation
			space might be rented
			protect the system from rogue programs allocating space
		hard limits may not be exceeded
		soft limits generate warnings
			number of warnings may be limited
			generate too many warnings => treat as hard limit
	Backing up a file system
		goal: create an extra copy of each file in the file system
			protect against disk failure
			protect against human error
			allow the system to track changes over time
		two basic types of backups
			full backup: make a copy of every file in the system
			incremental backup: only make a copy of files that have changed since
			the last backup
			faster: fewer files to copy
			smaller
		incremental backups typically require a full backup as a “base”
	Backup mechanics
		actually copy blocks from one file system to another
			safe if original FS fails
			safe if original FS is corrupted
			may be difficult to find modified files
			somewhat slow
		snapshot
			new data doesn’t overwrite old data
			easy to recover “deleted” files
			fast
			not as helpful for failed devices or corrupted FS
			snapshots can be done with hard links
	Finding files to back up
		"walk" the file system tree looking for files modified since the last backup
			slow
			works on any file system
		scan the list of i-nodes looking for modified files
			faster (sequential access)
			FS-specific code (may require raw
			disk access)
		keep a log of changes the FS makes
			can be very fast
			requires help from the FS
	Checking a file system for consistency
		build “in use” map by traversing all file i-nodes
			add 1 to block entry if mentioned in block list
		free map built from free list (or just use free block bitmap if it exists)
		consistent - used and free lists inverted bits
		missing block - 0 in used, 0 in free
		duplicate block in free list - 0 in used, 2 in free
		duplicate block in multiple files - 2 in used, 0 in free
	Fixing inconsistencies
		block marked neither free nor in use: mark free
			no file is using it, so mark it free
		block marked both free and in use: mark in use
			best to give it to the file that’s using it
		block marked free more than once: mark free
			ensure that it’s only on the free list once
		block in use in two files: ask user?
			difficult to decide what to do in this case
			consistency checking is designed to avoid having this happen: other
			cases are easy to handle
		this is the reasoning behind ordered updates of data and metadata
	File system cache
		many files are used repeatedly
			option: read it each time from disk
			better: keep a copy in memory
		file system cache
			set of recently used file blocks
			keep blocks just referenced
			throw out old, unused blocks
				same kinds of algorithms as for virtual memory
				more effort per reference is OK: file references are a lot less frequent than
				memory references
		goal: eliminate as many disk accesses as possible
			repeated reads & writes
			files deleted before they’re ever written to disk
	
3/9/16:
	Managing the file cache: reads
		update block lists on every read (and write)
			affordable: update is fast even compared to read of cached data
			evict blocks from cache using LRU, LFU, aging, or other VM-like algorithms
		readahead: fetch blocks that might be needed soon
			example: read block n+1 after reading block n of a file
			may be very inexpensive: disk controller has the block in its own cache
			particularly helpful if file is accessed sequentially: keep track of
			sequentiality to decide whether to read ahead
			not very detrimental if the OS guesses wrong: only waste a bit of disk
			bandwidth, and often no seek
	Managing the file cache: writes
		buffer writes in cache (writeback cache)
		periodically go through cache writing back dirty blocks
			typically about every 30 seconds on Unix
		benefits
			multiple writes to a single file block are coalesced
			files that are created and deleted quickly are never written to disk
		cache can be flushed manually if needed: sync()
	Defragmenting disks
		files on disk are stored where there's space
		on a full disk, files can be fragmented: stored non-contiguously
			"older" disks can have lots of fragmented files
		solution: defragment the disk
			collect blocks of each file together and store them contiguously
			some file systems do this automatically
	Ext2 & Ext3: the Linux file system
		Ext2/3 were the standard Linux file system
			similar to BSD’s Fast File System (FFS)
		Ext3 is like Ext2, but with journaling
		Ext2/3 is designed to be
			flexible across a wide range of workloads
			(relatively) simple to implement
			reliable
			usable across a wide range of file system sizes from hundreds of
			megabytes to terabytes
		basic design hasn’t changed much in 20 years
			Ext4 is now in use
	Large-scale structure in Ext3
		| boot | block group 0 | block group 1 | ... |
		| super block | group descriptors | data block bitmap | i-node bitmap | i-node table | data blocks |
		size	1				x					1				1				m				n
		disk broken into x block groups
		each block group is a (mostly) self-contained unit
			files can span block groups, but doesn’t happen often
			most information is local to the block group
		bitmaps must fit into a single file system block, limiting the number of data
		blocks
		copies of the superblock and group descriptors are stored in every block
		group for reliability
	Superblock
		information about the file system as a whole
			file block size & number of blocks
			fragment size & number of fragments
			number of blocks / fragments / inodes per group
		updated when the file system is modified
			free block & free inode counters
			time of last mount and write
		superblock cached in memory, updated on disk frequently
		superblock copy kept in each block group
			guards against loss of the superblock to disk corruption
			makes it more convenient to write out a copy in the nearest
			location
	Group descriptor
		one descriptor for each group
		block numbers for
			block bitmap
			inode bitmap
			first inode table block
		counts (limited to 16-bit integers)
			free blocks in the group
			free inodes in the group
			directories in the group
		each block group has a copy of all group descriptors
	More on block groups
		block group limited to maximum of block_size × 8 blocks (so block
		bitmap fits in a single block)
			maximum size (8 KB blocks) is 512 MB
			may have any number of block groups per file system
		maximum of 64K inodes per block group
		size of inode region (in blocks) calculated by
		num_inodes / (block_size/128)
			this determines the number of data blocks as well
		all block groups must be the same size
	Inode contents
		inode contains a file's metadata
			ownership & permissions
			time stamps
			block pointers
		inodes stored in the inode region in the block group
			inode number maps to a particular group / offset for the inode
		soft link stored in inode if possible (using block pointers)
		file "hole": missing blocks in the middle of a large file
	Inode fields
		mode - type & access rights
		owner - file owner
		gid - group identifier
		blocks - number of data blocks
		size - length in bytes
		atime - last access time
		ctime - last time inode modified
		mtime - last time file contents modified
		link count - number of hard links
		flags - file flags
		pointers - pointers to data blocks (direct, single/double/triple indirect)
	More on Unix FFS and Linus Ext3
		first few block pointers kept in inode
			small files have no extra overhead for index blocks
			reading & writing small files is fast
		indirect structures only allocated if needed
		for 4 KB file blocks (common in Unix), max file sizes are
			48 KB in directory (usually 12 direct blocks)
			1024 × 4 KB = 4 MB of additional file data for single indirect
			1024 × 1024 × 4 KB = 4 GB of additional file data for double indirect
			1024 × 1024 × 1024 × 4 KB = 4 TB for triple indirect
		maximum of 5 accesses for any file block on disk
			1 access to read inode and 1 to read file block
			maximum of 3 accesses to index blocks
			usually much fewer (1–2) because inode in memory
	
3/11/16:
	Linux directories
		linux supports
			plain text directories
			hashed directories
			plain text more common, but slower
		directories just like regular files
		name length limited to 255 bytes
		hashed directories
			use extendible hashing to make lookup faster
			may be "sparse"
	Major issue: consistency
		creating a file can require 5 disk writes
			a crash in the middle can leave the file system in an inconsistent state
			example: inode allocated, but not inserted into directory
		solution: use journaling, as provided in Ext3
		problem: what do you journal?
			metadata and data?
			metadata only?
				this introduces an ordering constraint
				data must be written before metadata is written
	Journaling in Ext3
		Ext3 uses a journal to record operations before they are written to disk
		three journaling modes
			journal: record everything to the journal
				slowest
				records data and metadata as soon as they’re written
			ordered: record only metadata, but order data and metadata writes so
			there’s no inconsistency
				faster
				less safe: metadata updates not committed immediately
				default mode for Ext3
			metadata only: record only metadata
				not as safe as other two
				fastest of the three
	Ext3 journal details
		log record: basic entry in journal
			one per disk request
			span of bytes, or entire block
		atomic operation handle: used to group log records together
			typically, one handle corresponds to a single system call
			ensures that high-level operation either completes or never
			happens (no partial operations)
		transaction: consists of multiple atomic operations
			done for efficiency reasons
			entire transaction written out at once
			only one transaction "open" at any time
	FreeBSD vs. Ext3
		freeBSD inodes have very similar structures
			set of direct block pointers, and one each single, double, triple indirect pointers
			similar flags
			no extents, but supports per-file specification of "block size"
				larger sizes require more blocks allocated consecutively
			there are some small differences
		freeBSD allocates inodes dynamically
			each inode block holds 128 inodes
			two allocated initially
			additional inode blocks allocated as needed
				potential inode blocks held as free until all other space is used
		freeBSD uses soft updates to ensure that updates are written in an order that preserves
		dependencies
			never point to a structure before it has been initialized
			never reuse a resource before nullifying all previous pointers to it
			never reset the old pointer to a live resource before the new pointer has been set
		freeBSD supports journaling of metadata operations
		freeBSD supports snapshots by creating a "snapshot file" to save information that may be
		overwritten by ongoing activity
	What's new in ext4
		new features
			extents
				handled by a separate two-level extent tree: pointer to it is in the inode
				need to ensure backwards compatibility
			option for larger inodes
			more scalable
			faster extended attributes
		Ext4 somewhat compatible with ext2/3
			mounts old file systems
			mountable under ext2/3 if no new features (i.e., extents) used
	CD-ROM file system
		file catalog in first few blocks of disk
			lists all files on the CD
			hierarchical structure can be built by treating files as directories (ISO 9660)
		files all allocated contiguously
			no need to handle overwrites
		current standard (UDF) is a superset of this
		| directory entry length | extended attribute record length | location of file | file size |
		  date and time | flags | interleave | CD# | L | basename . ext; ver | padding |
	MS-DOS (FAT) file system
		originally designed for floppy disks
			originally designed for small (5 MB) hard drives
			commonly used for thumb drives and SD cards
		single list of blocks in use (FAT => File Allocation Table)
			fixed overhead per block
			easy to find all blocks in a file
		root directory “file” starts in block 0
			other files have first block number in directory entry
	MS-DOS File Allocation Table sizes
		block size	fat-12	fat-16	fat-32
		0.5KB		2MB
		1KB			4MB
		2KB			8MB		128MB
		4KB			16MB	256MB	1TB
		8KB					512MB	2TB
		16KB				1024MB	2TB
		32KB				2048MB	2TB
	Directory entry in FAT
		| filename | extension | attributes | reserved | time | date | first block number | size |
		bytes 8			3			1			10		 2		2		2					4
	Storing a long name in VFAT
		long name stored in Windows 98 so that it’s backwards compatible with short names
			short name in “real” directory entry
			long name in “fake” directory entries: ignored by older systems
		OS designers will go to great lengths to make new systems work with older systems
	Zettabyte File System (ZFS)
		designed to handle very large disk partitions
		extensive error checking and recovery
			checksums on individual blocks
			RAID at the file level (RAIDZ)
		never overwrites data in place: no chance of inconsistent state
			write new blocks to disk into previously-free blocks
			write a new uberblock to reference the new file system state
			transition from one consistent state to another is done with a single block write
		some properties similar to log-structured file systems
			no overwrite-in-place
			needs cleaning
			but can write anywhere on disk: no need for segments
	Virtual file system switch
		how can the OS use multiple file systems?
			HFS+ / ext3 / NTFS
			VFAT
			ISO 9660 / UDF
		much code is common between them
			directory lookups
			caching
		solution: virtual file system (VFS) switch
			abstracts out common functionality
			provides default routines for many common functions
	Layering file systems
		often useful to layer one file system on top of another one
			use the underlying file system for many operations
			add (or enhance) functionality in the layered file system
		example: stacking file systems in FreeBSD
			nullfs: stacked file system does nothing
			FUSE: file system intercepts calls and lets a user-space program handle them
				this makes it easier to implement "interesting" functionality with a file system
				interface
