2/29/16:
	Disk zones
		outside tracks are longer than inside tracks
		two options for longer tracks
			bits are "bigger"
			more bits (transfer time)
		modern drives use second option
			more data on outer tracks
		disk divided into zones
			constant sectors per track in each zone
			8-30 zones on a disk
	Disk addressing
		millions of sectors on disk must be labelled
		two possibilities
			cylinder/track/sector
			sequential numbering
		modern drives use sequential numbers
			disks map sequential numbers into specific location
			mapping may be modified by disk
				remap bad sectors
				optimize performance
			hide exact geometry
	Build better "disk"
		CPU performance increasing exponentially,
		but disk performance hasn't
		disks aren't reliable
		solution - distribute data across disks and use
		some space to improve reliability
			data transferred in parallel
			data stored across drives (striping)
			parity on "extra" drive for reliability
	Redundant Array of Inexpensive Disks (RAID)
		RAID 0 - striped
		RAID 1 - mirrored copies
		RAID 4 - striped with parity
		RAID 5 - parity rotates through disks		
	CD-ROM recording
		CD-ROM has data in spiral
		one continuous track
		pits and lands "simulated" with heat-sensitive material
	Structure of a disk sector
		preamble | data | ECC
		preamble contains information about sector
			sector number and location information
		data usually 256, 512, or 1024 bytes
		ECC (Error Correcting Code) used to detect
		and correct minor errors in the data
	Sector layout on disk
		sectors numbered sequentially on each track
		numbering starts in different place on each track
			sector skew
			allows time for switching head from track to track
	Sector interleaving
		on older systems CPU slow => time elapsed between reading
		consecutive sectors
		solution - leave space between consecutively numbered sectors
		not done much any more
	What's in a disk request?
		time required to read or write a disk block
		determined by 3 factors
			seek time
			rotational delay
				average delay = 1/2 rotation time
			actual transfer time
				time to rotate over a sector
		seek time dominates, with rotation time close
		error checking done by controllers
	Disk request scheduling
		use hardware efficiently
			bandwidth as high as possible
			disk transferring as often as possible (not seeking)
		minimize disk seek time - moving from track to track
		minimize rotational latency - waiting for disk to
			rotate desired sector under read/write head
		calculate disk bandwidth by
			total bytes transferred / time to service request
			seek time and rotational latency are overhead
		minimize seek time and rotational latency by
			using algorithms to find a good sequence for servicing requests
			placing blocks of a file "near" each other
	Disk scheduling algorithms
		schedule disk requests to minimize disk seek time
			seek time increases as distance increases (not linearly)
			minimize seek distance => minimize seek time
	First-Come-First-Served (FCFS)
		requests serviced in the order which they arrived
			easy to implement
	Shortest Seek Time First (SSTF)
		service the request with the shortest seek time from the current
		head position
			form of SJF scheduling
			may starve some requests
	SCAN (elevator algorithm)
		disk arm starts at one end of the disk and moves towards the
		other end, servicing requests as it goes
	C-SCAN
		Identical to SCAN, except head returns to cylinder 0 when it
		reaches the end of the disk
	C-LOOK
		Identical to C-SCAN, except the head only travels as far as the last
		request in each direction
	Picking a disk scheduling algorithm
		SSTF is easy to implement and works OK if there aren't too many
		disk requests in the queue
		SCAN-type algorithms perform better for systems under heavy load
		Long seeks aren't too expensive, so choose C-LOOK over LOOK to
		make response time more even
		Disk request scheduling interacts with algorithms
	Flash memory
		compared to disk, flash is
			faster (shorter access time, but lower bandwidth)
			more expensive
			more reliable (devices)
			less reliable (sectors)
		compared to DRAM, flash is
			cheaper
			non-volatile (data survives power loss)
			slower
		use flash as a level between disk and memory?
		issues
			can't overwrite sectors "in place"
			flash wears out: can only write so many times per memory cell
	Writing to flash memory
		can't overwrite page in place
			need to write to clean region
			unit of erase is erase block
		copy entire erase block in new location
			modify the page being written
	Is there a better way to
	Handling flash in the OS
		treat it like a disk
			flash written in blocks, just like a disk
			flash is random access, just like disk
		need to be careful about wearing out flash
			most flash devices do "wear levelling" - remap blocks
			internally when they're erased
	
3/2/16:
	Clock hardware
		crystal oscillator with fixed frequency (only when computer on)
			typically used to time short intervals (~1 second)
			may be used to correct time-of-day clock
		time-of-day clock (runs when system off)
			keeps minutes, hours, days
			not to accurate
			used to load system clock at start-up
		time kept in seconds and ticks (often 100-1000 per second)
			number of seconds since particular time (UNIX: January 1, 1970)
			number of ticks this second
			advance ticks once per clock interrupt
			advance seconds when ticks "overflow"
	Keeping time
		check time via the web
			protocol to get time from accurate time servers
		when system clock slow
			advance to correct time
			may be done all at once or over a minute or two
		when system clock fast
			can't have backwards time
			advance time more slowly than normal until clock correct
		track clock drift, do periodic updates to keep accuracy
	Using timers in software
		use short interval clock for timer interrupts
			specified by applications
			no problems if interrupt frequency is low
			may have multiple timers using single clock chip
		use soft timers to avoid interrupts
			kernel check for soft timer expiration before it
			exits to user mode
			less accurate than hardware timer
			how well this work depends on rate of kernel entries
	Where does the power go?
		most power to laptop display
		save power by
			reduce display power
			slow down CPU
			reduce disk power
	Reducing CPU power usage
		running at full clock speed
			jobs finish quickly
			high energy consumption
		cut voltage by half
			50% clock speed halved
			25% power consumption
			50% energy consumption (power * time)
	Long-term information storage
		store large amounts of data
			gigabytes < terabytes < petabytes
		stored information must survive termination of process
		using it
		multiple processes must access the information concurrently
	Naming files
		every file has at least one name
		name is human-accessible or machine-usable (e.g, "foo.text", or 192837)
		case may or may not matter
		name may include information about files' content
			computer may use part of name to determine file type
	Internal file structure
		executable file
			header
				magic number
				text size
				data size
				BSS size
				symbol table size
				entry point
				flags
			text
			data
			relocation bits
			symbol table
		archive file
			header
				module name
				date
				owner
				protection
				size
			object module
			header
			object module
	Accessing a file
		sequential access
			read all bytes/records from the beginning
			cannot jump around (may rewind)
			convenient when medium was magnetic tape
			often useful when whole file is needed
		random access
			bytes (or records) read in any order
			essential for database systems
			read can be
				move file marker (seek), then read
				read and then move file marker
	File attributes
		protection, password, creator, owner, read-only flag, hidden flag,
		system flag, archived flag, ASCII/binary flag, random access flag,
		temporary flag, lock flags, record length, key position/length,
		creation time, last access time, last modified time, current size,
		maximum size
	File operations
		create - make new file
		delete - remove existing file
		open - prepare file to be accessed
		close - indicate that a file no longer being accessed
		read - get data from file
		write - put data to a file
		append - like write, but only at the end of a file
		seek - move the current pointer elsewhere in the file
		get attributes - retrieve attribute information
		set attributes - modify attribute information
		rename - change a file's name
	Memory-mapped files
		segmented process before mapping files into its address space
		process after mapping
			existing file foo into one segmented
			create new segment for bar
		memory mapped files are a convenient abstraction
			e.g., string search in large file can be done just as with memory
			let OS do the buffering in virtual memory system
		some issues
			how long is file
				easy for read-only
				difficult for write-enabled
			what if file is shared
			when are writes flushed to disk
		easier to memory map read-only files
	Directories
		name is limited
		humans organize thing in groups
		groups allow
			find files easier
			locate related files
	Single-level directory systems
		one directory in the file system
		problem: file naming conflicts (especially across users)
	Two-level directory system
		solves naming problem - each user had their own directory
		extension: allow users to access files in other's directories
	Hierarchical directory system
		Allow multiple levels of files under directories with different user
		access levels
	
3/4/16:
	Operations on directories
		create - make a new directory
		delete - remove a directory (usually must be empty)
		opendir - open a directory to allow searching it
		closedir - close a directory (done searching)
		readdir - read a directory entry
		rename - change the name of a directory
		link - create a new entry in a directory to link and existing file
		unlink - remove an entry in a directory
			remove the file if this is the last link to this file
	File system implementation issues
		how are disks divided into file systems
		how does file system allocate blocks to files
		how does file system manage free space
		how are directories handled
		how to improve performance and reliability
	Carving up the disk
		disk
			master boot record | partition table | partitions ...
		partition
			boot block | super block | free space management| index nodes | files & directories
	Contiguous allocation for file blocks
		requires all blocks of a file to be consecutive on disk
		problem - deleting files leaves "holes"
			similar to memory allocation issues
			compacting disk may be very slow procedure
	Contiguous allocation
		data in each file is stored in consecutive blocks on disk
		simple and efficient indexing
			starting location (block #) on disk
			length of the file in blocks
		random access well-supported
		difficult to grow files
			must pre-allocate all needed space
			wasteful of storage if file isn't using all space
		logical to physical mapping is easy
			blocknum = (pos/1024) + start;
			offset_in_block = pos % 1024;
	Linked allocation
		file is a linked list of disk blocks
			blocks may be scattered around disk drive
			block contains both pointer to next block and data
			files may be as long as needed
		new blocks are allocated as needed
			linked into list of blocks in file
			removed from list (bitmap) of free blocks
	Finding blocks with linked allocation
		directory structure is simple
			starting address looked up from directory
			directory only keeps track of first block
		no wasted space - all blocks can be used
		random access is difficult - must always start at first block
		logical to physical mapping
			block = start;
			offset_in_block = pos % 1020;
			for (j = 0; j < pos / 1020; j++) {
				block = block->next;
			}
	Linked allocation using a table in RAM
		links on disk are slow
		keep linked list in memory
		advantage - faster
		disadvantages
			have to copy it to disk at some point
			have to keep in-memory and on-disk copy consistent
	Using a block index for allocation
		store file block address in array
			array stored in disk block
			directory has pointer to this block
			non-existent blocks indicated by -1
		random access easy
		limit on file size?
	Finding blocks with indexed allocation
		need location of index table - lookup in directory
		random and sequential access both well-supported
			- lookup block number in index table
		space utilization is good
			no wasted blocks
			files can grow and shrink easily
			overhead of single block per file
		logical to physical mapping
			block = index[pos / 1024];
			offset_in_block = pos % 1024;
		limited file size - 256 pointers per index block,
		1KB per file block => 256 KB per file limit
	Larger files with indexed allocation
		linked index blocks
		logical to physical mapping
			index = start;
			blocknum = pos / 1024;
			for (j= 0; j < blocknum / 255; j++) {
				index = index->next;
			}
			block = index[blocknum % 255];
			offset_in_block = pos % 1024;
		file size now unlimited
		random access slow, but only for very large files
	Two-level indexed allocation
		allow larger files by creating an index of index blocks
			file size still limited, but much larger
			limit for 1 KB blocks = 1 KB x 256 x 256 = 2^26 bytes = 64 MB
		logical to physical mapping
			blocknum = pos / 1024;
			index = start[blocknum / 256];
			block = index[blocknum % 256];
			offset_in_block = pos % 1024;
			start is only pointer kept in directory
			overhead is now at least two blocks per file
	Block allocation with extents
		reduce space consumed by index pointers
			often consecutive blocks in file are sequential on disk
			store <block,count> instead of <block> in index
			at each level, keep total count for the index
		lookup procedure
			- find correct index block by checking the starting file offset
			for each index block
			- find correct <block,count> entry by running through index
			block, keeping track of how far into file entry is
			- find correct block in <block,count> pair
		more efficient if file blocks tend to be consecutive on disk
	Managing free space - bit vector
		keep a bit vector, with one entry per file block
			number bits from 0 through n-1, where n is the number
			of blocks available for files on the disk
			if bit[j] == 0, block j is free
			if bit[j] == 1, block j is in use (data or index)
		if words are 32 bits, calculate appropriate bit by
			wordnum = block / 32;
			bitnum = block % 32;
		search for free blocks by looking for words with bits unset
		(words != 0xffffff)
		easy to find consecutive blocks for a single file
		bit map must be stored on disk, and consumes space
			assume 4 KB blocks, 256 GB disk => 64 M blocks
			64 M bits = 2^26 bits = 2^23 bytes = 8 MB overhead
	Managing free space - linked list
		use a linked list to manage free blocks
			no wasted space for bitmap
			no need for random access unless we want to find
			consecutive blocks for a single file
		- difficult to know how many blocks are free unless its
		tracked elsewhere in file system
		- difficult to group nearby blocks together if they're
		freed at different times
			less efficient allocation of blocks
			files read and write more because consecutive blocks not nearby
